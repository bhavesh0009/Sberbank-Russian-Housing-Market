{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38133, 305)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv(\"C:\\\\Users\\\\user\\\\AnacondaProjects\\\\Sberbank\\\\train.csv\", parse_dates=['timestamp'])\n",
    "test =  pd.read_csv(\"C:\\\\Users\\\\user\\\\AnacondaProjects\\\\Sberbank\\\\test.csv\", parse_dates=['timestamp'])\n",
    "macro = pd.read_csv(\"C:\\\\Users\\\\user\\\\AnacondaProjects\\\\Sberbank\\\\macro.csv\", parse_dates=['timestamp'])\n",
    "\n",
    "#selecting only important macro columns\n",
    "macro = macro[[\"timestamp\",\"balance_trade\",\"balance_trade_growth\",\"eurrub\",\"average_provision_of_build_contract\",\"micex_rgbi_tr\",\"micex_cbi_tr\",\"deposits_rate\",\"mortgage_value\",\"mortgage_rate\",\"income_per_cap\",\"rent_price_4+room_bus\",\"museum_visitis_per_100_cap\",\"apartment_build\"]]\n",
    "\n",
    "y_train = train['price_doc']\n",
    "id_test = test['id']\n",
    "\n",
    "train.drop(['id', 'price_doc'], axis=1, inplace=True)\n",
    "test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# Build all_data = (train+test).join(macro)\n",
    "num_train = len(train)\n",
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.join(macro, on='timestamp', rsuffix='_macro')\n",
    "\n",
    "'''\n",
    "# Add month-year\n",
    "month_year = (all_data.timestamp.dt.month + all_data.timestamp.dt.year * 100)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "all_data['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "\n",
    "# Add week-year count\n",
    "week_year = (all_data.timestamp.dt.weekofyear + all_data.timestamp.dt.year * 100)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "all_data['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "'''\n",
    "\n",
    "# Creating Apartment Name Feature\n",
    "all_data['apartment_name'] = pd.factorize(all_data.sub_area + all_data['metro_km_avto'].astype(str))[0]\n",
    "\n",
    "#cleaning of full_sq\n",
    "all_data.loc[((all_data[\"full_sq\"]<=6) | (all_data[\"full_sq\"]>300)) & (all_data[\"life_sq\"]>=6) & (all_data[\"life_sq\"]<300) & ((all_data[\"full_sq\"]>= all_data[\"life_sq\"]*15) | ((all_data[\"full_sq\"]>=0) & (all_data[\"full_sq\"]<all_data[\"life_sq\"]))),\"full_sq\"]=all_data[((all_data[\"full_sq\"]<=6) | (all_data[\"full_sq\"]>300)) & (all_data[\"life_sq\"]>=6) & (all_data[\"life_sq\"]<300) & ((all_data[\"full_sq\"]>= all_data[\"life_sq\"]*15) | ((all_data[\"full_sq\"]>=0) & (all_data[\"full_sq\"]<all_data[\"life_sq\"])))].life_sq\n",
    "g_Apartment_col=all_data.groupby('apartment_name')['full_sq'].agg(['mean','median','count']).reset_index()\n",
    "g_Apartment_col.columns= ['apartment_name','full_sq_mean','full_sq_median','apartment_count'] \n",
    "all_data=all_data.merge(g_Apartment_col, how='left')\n",
    "all_data.loc[((all_data[\"full_sq\"]<=6) | (all_data[\"full_sq\"]>300)) & (all_data.apartment_count>3),\"full_sq\"]=all_data[((all_data[\"full_sq\"]<=6) | (all_data[\"full_sq\"]>300)) & (all_data.apartment_count>3)].full_sq_mean\n",
    "\n",
    "all_data.drop(\"full_sq_median\", axis=1, inplace=True)\n",
    "all_data.drop(\"full_sq_mean\", axis=1, inplace=True)\n",
    "\n",
    "# Remove timestamp column (may overfit the model in train)\n",
    "all_data.drop(['timestamp', 'timestamp_macro'], axis=1, inplace=True)\n",
    "\n",
    "# cleaning price_doc\n",
    "train = all_data[:num_train]\n",
    "test  = all_data[num_train:]\n",
    "\n",
    "train = pd.concat((train,y_train),axis=1)\n",
    "train[\"prize_per_sqrmtr\"]=train.price_doc/train.full_sq\n",
    "# grouping by price_doc\n",
    "g_Apartment_col=train.groupby('apartment_name')['prize_per_sqrmtr'].agg(['mean','std']).reset_index()\n",
    "g_Apartment_col.columns= ['apartment_name','pps_mean','pps_std'] \n",
    "train=train.merge(g_Apartment_col, how='left')\n",
    "# change value if more than 4 sigma\n",
    "train.loc[(train.prize_per_sqrmtr > train.pps_mean + (4*train.pps_std)) | (train.prize_per_sqrmtr < train.pps_mean - (4*train.pps_std)),\"prize_per_sqrmtr\"]=train[(train.prize_per_sqrmtr > train.pps_mean + (4*train.pps_std)) | (train.prize_per_sqrmtr < train.pps_mean - (4*train.pps_std))].pps_mean\n",
    "# grouping by sub_area\n",
    "g_Apartment_col=train.groupby('sub_area')['prize_per_sqrmtr'].agg(['mean']).reset_index()\n",
    "g_Apartment_col.columns= ['sub_area','sa_mean'] \n",
    "train=pd.merge(train, g_Apartment_col, how='left', on='sub_area')\n",
    "# Change if out of range\n",
    "train.loc[ (train.prize_per_sqrmtr>=600000) | (train.prize_per_sqrmtr<=10000)\t,\"prize_per_sqrmtr\"]= train[(train.prize_per_sqrmtr>=600000) | (train.prize_per_sqrmtr<=10000)].sa_mean\n",
    "train[\"price_doc\"]=train.prize_per_sqrmtr*train.full_sq\n",
    "y_train = train['price_doc'].values\n",
    "# Dropping grouping columns\n",
    "train.drop(['pps_mean','pps_std','sa_mean','prize_per_sqrmtr','price_doc'], axis=1, inplace=True)\n",
    "\n",
    "all_data = pd.concat([train, test])\n",
    "\n",
    "# cleaning life_sq\n",
    "all_data.loc[(all_data.life_sq.isnull()) | (all_data.life_sq<6) | (all_data.life_sq>all_data.full_sq) | (all_data.life_sq>224),\"life_sq\"] =all_data[(all_data.life_sq.isnull()) | (all_data.life_sq<6) | (all_data.life_sq>all_data.full_sq) | (all_data.life_sq>224)].full_sq/1.66\n",
    "\n",
    "# cleaning floor\n",
    "all_data.loc[(all_data.floor>48),\"floor\"]=np.NaN\n",
    "\n",
    "# cleaning max_floor\n",
    "g_Apartment_col=all_data.groupby('apartment_name')['floor'].agg(['max']).reset_index()\n",
    "g_Apartment_col.columns= ['apartment_name','apartment_floor_max'] \n",
    "all_data=all_data.merge(g_Apartment_col, how='left')\n",
    "all_data.loc[((all_data.max_floor.isnull()) | (all_data.max_floor>48)) & (all_data.apartment_count>3),\"max_floor\"] = all_data[((all_data.max_floor.isnull()) | (all_data.max_floor>48)) & (all_data.apartment_count>3)].apartment_floor_max\n",
    "all_data.drop(['apartment_floor_max'], axis=1, inplace=True)\n",
    "all_data.loc[((all_data.max_floor.isnull()) | (all_data.floor>0)),\"max_floor\"]=all_data[(all_data.max_floor.isnull()) | (all_data.floor>0)].floor+1\n",
    "\n",
    "# cleaning build_year\n",
    "g_Apartment_col=all_data.groupby('apartment_name')['build_year'].agg(['median']).reset_index()\n",
    "g_Apartment_col.columns= ['apartment_name','byear_mean']\n",
    "all_data=pd.merge(all_data, g_Apartment_col, how='left', on='apartment_name')\n",
    "all_data.loc[(all_data.build_year<1950) | (all_data.build_year>2020) , \"build_year\"]=all_data[(all_data.build_year<1950) | (all_data.build_year>2020)].byear_mean\n",
    "all_data.drop(['byear_mean'], axis=1, inplace=True)\n",
    "\n",
    "# cleaning num_room\n",
    "g_Apartment_col=all_data.groupby('apartment_name')['num_room'].agg(['median']).reset_index()\n",
    "g_Apartment_col.columns= ['apartment_name','nrmedian']\n",
    "all_data=pd.merge(all_data, g_Apartment_col, how='left', on='apartment_name')\n",
    "all_data.loc[((all_data.num_room>=7) & (all_data.full_sq<100)) | (all_data.num_room==0) | (all_data.num_room.isnull()),\"num_room\"]=all_data[((all_data.num_room>=7) & (all_data.full_sq<100)) | (all_data.num_room==0) | (all_data.num_room.isnull())].nrmedian\n",
    "all_data.drop(['nrmedian'], axis=1, inplace=True)\n",
    "\n",
    "all_data['rel_kitch_sq'] = all_data['kitch_sq'] / all_data['life_sq'].astype(float)\n",
    "\n",
    "\n",
    "# cleaning kitch_sq\n",
    "g_Apartment_col=all_data.groupby('apartment_name')['rel_kitch_sq'].agg(['median']).reset_index()\n",
    "g_Apartment_col.columns= ['apartment_name','rksmedian']\n",
    "all_data=pd.merge(all_data, g_Apartment_col, how='left', on='apartment_name')\n",
    "all_data.loc[((all_data.rel_kitch_sq>1) | (all_data.kitch_sq==0) |(all_data.kitch_sq>60) | (all_data.kitch_sq.isnull())) & (all_data.apartment_count>=5),\"kitch_sq\" ]=all_data[((all_data.rel_kitch_sq>1) | (all_data.kitch_sq==0) |(all_data.kitch_sq>60) | (all_data.kitch_sq.isnull())) & (all_data.apartment_count>=5)].life_sq * all_data[((all_data.rel_kitch_sq>1) | (all_data.kitch_sq==0) |(all_data.kitch_sq>60) | (all_data.kitch_sq.isnull())) & (all_data.apartment_count>=5)].rksmedian\n",
    "all_data.loc[(all_data.rel_kitch_sq>1) | (all_data.kitch_sq==0) |(all_data.kitch_sq>60) | (all_data.kitch_sq.isnull()),\"kitch_sq\"]=all_data[(all_data.rel_kitch_sq>1) | (all_data.kitch_sq==0) |(all_data.kitch_sq>60) | (all_data.kitch_sq.isnull())].life_sq * 0.23\n",
    "all_data.drop(['rksmedian'], axis=1, inplace=True)\n",
    "\n",
    "# cleaning state\n",
    "g_Apartment_col=all_data[all_data.state>=0].groupby('apartment_name')['state'].agg(['median','mean']).reset_index()\n",
    "g_Apartment_col.columns= ['apartment_name','state_median','state_mean']\n",
    "all_data=pd.merge(all_data, g_Apartment_col, how='left', on='apartment_name')\n",
    "all_data.loc[((all_data.state.isnull())|(all_data.state>4))&(all_data.apartment_count>=5)&(all_data.state_median>=0) ,\"state\"]=all_data[((all_data.state.isnull())|(all_data.state>4))&(all_data.apartment_count>=5) & (all_data.state_median>=0)].state_median.astype(int)\n",
    "all_data.drop(['state_median'], axis=1, inplace=True)\n",
    "all_data.drop(['state_mean'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# cleaning material\n",
    "g_Apartment_col=all_data[all_data.material>=0].groupby('apartment_name')['material'].agg(['median','mean']).reset_index()\n",
    "g_Apartment_col.columns= ['apartment_name','mat_median','mat_mean']\n",
    "all_data=pd.merge(all_data, g_Apartment_col, how='left', on='apartment_name')\n",
    "all_data.loc[((all_data.material.isnull())|(all_data.material>6))&(all_data.apartment_count>=5)&(all_data.mat_median>=0) ,\"material\"]=all_data[((all_data.material.isnull())|(all_data.material>6))&(all_data.apartment_count>=5) & (all_data.mat_median>=0)].mat_median.astype(int)\n",
    "all_data.drop(['mat_median'], axis=1, inplace=True)\n",
    "all_data.drop(['mat_mean'], axis=1, inplace=True)\n",
    "all_data.drop(['apartment_name'], axis=1, inplace=True)\n",
    "\n",
    "all_data['rel_floor'] = all_data['floor'] / all_data['max_floor'].astype(float)\n",
    "all_data['rel_kitch_sq'] = all_data['kitch_sq'] / all_data['full_sq'].astype(float)\n",
    "\n",
    "\n",
    "factorize = lambda t: pd.factorize(t[1])[0]\n",
    "\n",
    "df_obj = all_data.select_dtypes(include=['object'])\n",
    "\n",
    "X_all = np.c_[\n",
    "    all_data.select_dtypes(exclude=['object']).values,\n",
    "    np.array(list(map(factorize, df_obj.iteritems()))).T\n",
    "]\n",
    "\n",
    "X_train = X_all[:num_train]\n",
    "X_test = X_all[num_train:]\n",
    "\n",
    "\n",
    "\n",
    "# Deal with categorical values\n",
    "df_numeric = all_data.select_dtypes(exclude=['object'])\n",
    "df_obj = all_data.select_dtypes(include=['object']).copy()\n",
    "\n",
    "for c in df_obj:\n",
    "    df_obj[c] = pd.factorize(df_obj[c])[0]\n",
    "\n",
    "df_values = pd.concat([df_numeric, df_obj], axis=1)\n",
    "\n",
    "# Convert to numpy values\n",
    "X_all = df_values.values\n",
    "print(X_all.shape)\n",
    "\n",
    "X_train = X_all[:num_train]\n",
    "X_test = X_all[num_train:]\n",
    "\n",
    "df_columns = df_values.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:8.19549e+06\ttest-rmse:8.20309e+06\n",
      "[20]\ttrain-rmse:3.81865e+06\ttest-rmse:4.01389e+06\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1,\n",
    "    'booster' :'gbtree',\n",
    "    'tuneLength': 3\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)\n",
    "dtest = xgb.DMatrix(X_test, feature_names=df_columns)\n",
    "\n",
    "cv_result = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n",
    "    verbose_eval=20, show_stdv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_boost_rounds = len(cv_result)\n",
    "cv_result[['train-rmse-mean', 'test-rmse-mean']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 16))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_pred = model.predict(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmsle(x_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(dtest)\n",
    "\n",
    "df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n",
    "\n",
    "df_sub.to_csv('subxgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
